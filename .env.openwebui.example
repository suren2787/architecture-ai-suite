# ===================================================================
# OpenWebUI Configuration Example
# ===================================================================
# This is a sample configuration for using OpenWebUI as a proxy
# to AWS Bedrock or other LLM providers.
#
# Use this when you don't have direct access to AWS Bedrock but have
# an OpenWebUI instance that acts as a proxy.
#
# Copy this file to .env and update with your actual credentials.
# ===================================================================

# ===================================================================
# LLM Provider Configuration
# ===================================================================
MODEL_PROVIDER=openwebui
MODEL_NAME=deepseek-r1

# Available model names depend on your OpenWebUI configuration.
# Common examples:
#   - deepseek-r1 (if Bedrock DeepSeek-R1 is configured in OpenWebUI)
#   - claude-3-sonnet (if Anthropic Claude is configured)
#   - gpt-4 (if OpenAI GPT-4 is configured)
#   - gpt-3.5-turbo
#
# Check your OpenWebUI instance for available models

# ===================================================================
# OpenWebUI API Configuration
# ===================================================================
OPENWEBUI_API_KEY=your-openwebui-api-key-here
OPENWEBUI_BASE_URL=https://your-openwebui-instance.com/api/v1

# How to get your API key:
# 1. Log into your OpenWebUI instance
# 2. Go to Settings → Account → API Keys
# 3. Click "Generate New API Key"
# 4. Copy the key and paste it above
#
# Base URL format:
# - Should end with /api/v1
# - Examples:
#   - https://openwebui.company.com/api/v1
#   - http://localhost:8080/api/v1 (for local development)

# ===================================================================
# Embeddings Configuration
# ===================================================================
EMBEDDING_PROVIDER=openwebui
EMBEDDING_MODEL=text-embedding-3-small

# You can also use other embedding providers if needed:
# Option 1: OpenWebUI (recommended if available)
#   EMBEDDING_PROVIDER=openwebui
#   EMBEDDING_MODEL=text-embedding-3-small
#
# Option 2: HuggingFace (free, local, no API needed)
#   EMBEDDING_PROVIDER=huggingface
#   EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
#
# Option 3: Direct OpenAI (if you have OpenAI API key)
#   EMBEDDING_PROVIDER=openai
#   EMBEDDING_MODEL=text-embedding-3-small
#   OPENAI_API_KEY=sk-your-openai-key

# ===================================================================
# Confluence Integration (Optional)
# ===================================================================
# CONFLUENCE_EMAIL=your-email@company.com
# CONFLUENCE_API_TOKEN=your-confluence-api-token
# CONFLUENCE_BASE_URL=https://yourcompany.atlassian.net/wiki

# ===================================================================
# Notes
# ===================================================================
# 1. This configuration uses OpenWebUI for both LLM and embeddings
# 2. No AWS credentials needed (OpenWebUI handles the AWS connection)
# 3. Make sure your OpenWebUI instance has the models configured
# 4. Test connectivity with: curl -H "Authorization: Bearer YOUR_KEY" YOUR_BASE_URL/models
